{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaZKpPC43q_D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "!pip install -q \"flash-attn==2.8.3\" --no-build-isolation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3e435ec35b394d9a8ed3afe76cbe3b23",
            "cf179b051a8a4fc380be321f4aaf7c6b",
            "0eb1370d7c82457183d154800be61ea3",
            "7c66af747fa540b082a5f99ac50c563f",
            "d747c3cfbdb64c2d860c45bfc19af58f",
            "ca055a839512487d88a541d1f9d5c154",
            "f335411ea2aa4d11bc2692c0779d8810",
            "bbf3b6e62a984e2485fa844fb4195dd0",
            "2aa8b2acec0942679b99134233f0e44f",
            "afd50efe50b84c38bec39e4f95396940",
            "806930ae0f0748808050c7f2aad68f93",
            "3136f2e952f64b3b8805f216ce1f78d9",
            "7d7ab3a8b31546f6aa0052856e6036fe",
            "a558818de552481f86eb96c6b8904363",
            "eede73f988864d8f845ca49b11f80351",
            "5f90ef82fc694012942f569ae85584ad",
            "59eeaac680674844b574b03be58b3793",
            "73e3536f450c475da15c14a89fd3baa9",
            "92ffa3a0516a4542a73043a289374136",
            "7133b1db16c5459b8d097efc6757321c",
            "a453e3718a7542f481c2f0578d885101",
            "51fbab3f5d2d4c71ab001afafb1c76fe",
            "1df50ac2bc704d1fb89edecf154ded2f",
            "1b80c0d79bf846ccbb1529fc074a2c1a",
            "eab3e0dd104d4f9396ffb684c7e8c811",
            "97ef8e3cb10a42988472f302c386baf3",
            "0abe528b1a94430bb989d5c1d1edf29e",
            "0f8361d616204c70b2fea12c42642eda",
            "5707d2163bb848f3b59ad8e6e5321fd5",
            "56b0d80c9692490b8ac5154c1790c204",
            "e52fe43fd31b4803babde96e0d0b7344",
            "9cc62bc8c3484e3caeb9fbd63badd5e4",
            "492876adcfb5411386450b167c0cf98e",
            "6661bb0efd4c4f0f8be2a96aa61f8a74",
            "dc96402e48c84ee1ae898aa0846aec40",
            "603090355d8746648a5e9952962f8712",
            "3ef938db60944f95ba0c248fe97fefad",
            "ca284c8cb29344ea825dc8fbaac9cce8",
            "96829f52a8ae49798a1b6b9a239f8daa",
            "86b7ef5d84f6413f8f3fe905304f2c3b",
            "71f8b13bc7234deb88d7d318452ae9e5",
            "c2501b2ceb0b47b9b2c45dd65c5b9f9b",
            "2a32444f32c048dca764cca25b4eabd4",
            "4c924cc011cc418cb2bdb0f65b2009cc"
          ]
        },
        "id": "woUJf6mrDoNQ",
        "outputId": "8928bc43-6995-4f37-d6ef-0230674212fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "BASELINE (No Pruning)\n",
            "============================================================\n",
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e435ec35b394d9a8ed3afe76cbe3b23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention implementation: sdpa\n",
            "Original intermediate size: 14336\n",
            "Original attention heads: 32\n",
            "Original KV heads: 8\n",
            "\n",
            "Evaluating perplexity on wikitext...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing perplexity: 100%|██████████| 50/50 [00:01<00:00, 43.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating generation quality on 64 prompts (batch_size=64)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating batches: 100%|██████████| 1/1 [00:13<00:00, 13.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 19200, Total time: 13.93s, Throughput: 1378.58 tok/s\n",
            "\n",
            "--- BASELINE RESULTS ---\n",
            "Parameters: 8,030,261,248\n",
            "Perplexity: 15.46\n",
            "Throughput: 1378.58 tok/s\n",
            "\n",
            "============================================================\n",
            "PRUNING EXPERIMENT: MLP=25%, Attention=20%\n",
            "============================================================\n",
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3136f2e952f64b3b8805f216ce1f78d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention implementation: sdpa\n",
            "Original intermediate size: 14336\n",
            "Original attention heads: 32\n",
            "Original KV heads: 8\n",
            "\n",
            "======================================================================\n",
            "Pruning Model: MLP=25%, Attention=20%\n",
            "======================================================================\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "Layer 0:\n",
            "  MLP: 14336 -> 10752 neurons\n",
            "  Attention: 32 -> 24 heads (KV: 8 -> 6)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "Processed 8/32 layers...\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "Processed 16/32 layers...\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "Processed 24/32 layers...\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "    Aligning to 10752 neurons (was 10752)\n",
            "Processed 32/32 layers...\n",
            "\n",
            "======================================================================\n",
            "Pruning Complete!\n",
            "======================================================================\n",
            "MLP Neurons:      344,064/458,752 (75.0% kept)\n",
            "Attention Heads:  768/1024 (75.0% kept)\n",
            "Final MLP size:   10752\n",
            "Final Attention:  24 heads, 6 KV heads\n",
            "\n",
            "Evaluating perplexity on wikitext...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing perplexity: 100%|██████████| 50/50 [00:01<00:00, 43.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating generation quality on 64 prompts (batch_size=64)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating batches: 100%|██████████| 1/1 [00:13<00:00, 13.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 19200, Total time: 13.08s, Throughput: 1467.90 tok/s\n",
            "\n",
            "--- RESULTS ---\n",
            "Parameters: 21.7% reduction (6,285,430,784 params)\n",
            "Perplexity: 124.33 (+704.0% change)\n",
            "Throughput: 1467.90 tok/s (+6.5%)\n",
            "\n",
            "⚠️  WARNING: Model severely degraded (5x perplexity increase)!\n",
            "\n",
            "Sample generation:\n",
            "who said truth justice and the american way of life.\n",
            "The American Declaration of Independence.\n",
            "The American Declaration of Independence.\n",
            "The American Declaration of Independence.\n",
            "The American Declarat...\n",
            "\n",
            "============================================================\n",
            "PRUNING EXPERIMENT: MLP=40%, Attention=30%\n",
            "============================================================\n",
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1df50ac2bc704d1fb89edecf154ded2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention implementation: sdpa\n",
            "Original intermediate size: 14336\n",
            "Original attention heads: 32\n",
            "Original KV heads: 8\n",
            "\n",
            "======================================================================\n",
            "Pruning Model: MLP=40%, Attention=30%\n",
            "======================================================================\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "Layer 0:\n",
            "  MLP: 14336 -> 8448 neurons\n",
            "  Attention: 32 -> 20 heads (KV: 8 -> 5)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "Processed 8/32 layers...\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "Processed 16/32 layers...\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "Processed 24/32 layers...\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "    Aligning to 8448 neurons (was 8602)\n",
            "Processed 32/32 layers...\n",
            "\n",
            "======================================================================\n",
            "Pruning Complete!\n",
            "======================================================================\n",
            "MLP Neurons:      270,336/458,752 (58.9% kept)\n",
            "Attention Heads:  640/1024 (62.5% kept)\n",
            "Final MLP size:   8448\n",
            "Final Attention:  20 heads, 5 KV heads\n",
            "\n",
            "Evaluating perplexity on wikitext...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing perplexity: 100%|██████████| 50/50 [00:01<00:00, 43.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating generation quality on 64 prompts (batch_size=64)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating batches: 100%|██████████| 1/1 [00:13<00:00, 13.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 19200, Total time: 13.01s, Throughput: 1475.82 tok/s\n",
            "\n",
            "--- RESULTS ---\n",
            "Parameters: 35.1% reduction (5,211,688,960 params)\n",
            "Perplexity: 716.29 (+4532.3% change)\n",
            "Throughput: 1475.82 tok/s (+7.1%)\n",
            "\n",
            "⚠️  WARNING: Model severely degraded (5x perplexity increase)!\n",
            "\n",
            "Sample generation:\n",
            "who said truth justice and the american way.\n",
            "://://_REF to correct errors and the correct errors and correct errors and correct errors and correct errors and correct errors and correct errors and corr...\n",
            "\n",
            "============================================================\n",
            "PRUNING EXPERIMENT: MLP=60%, Attention=50%\n",
            "============================================================\n",
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6661bb0efd4c4f0f8be2a96aa61f8a74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention implementation: sdpa\n",
            "Original intermediate size: 14336\n",
            "Original attention heads: 32\n",
            "Original KV heads: 8\n",
            "\n",
            "======================================================================\n",
            "Pruning Model: MLP=60%, Attention=50%\n",
            "======================================================================\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Layer 0:\n",
            "  MLP: 14336 -> 5632 neurons\n",
            "  Attention: 32 -> 16 heads (KV: 8 -> 4)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Processed 8/32 layers...\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Processed 16/32 layers...\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Processed 24/32 layers...\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Processed 32/32 layers...\n",
            "\n",
            "======================================================================\n",
            "Pruning Complete!\n",
            "======================================================================\n",
            "MLP Neurons:      180,224/458,752 (39.3% kept)\n",
            "Attention Heads:  512/1024 (50.0% kept)\n",
            "Final MLP size:   5632\n",
            "Final Attention:  16 heads, 4 KV heads\n",
            "\n",
            "Evaluating perplexity on wikitext...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing perplexity: 100%|██████████| 50/50 [00:01<00:00, 43.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating generation quality on 64 prompts (batch_size=64)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating batches: 100%|██████████| 1/1 [00:13<00:00, 13.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 19200, Total time: 12.98s, Throughput: 1478.77 tok/s\n",
            "\n",
            "--- RESULTS ---\n",
            "Parameters: 51.0% reduction (3,936,620,544 params)\n",
            "Perplexity: 3236.87 (+20833.0% change)\n",
            "Throughput: 1478.77 tok/s (+7.3%)\n",
            "\n",
            "⚠️  WARNING: Model severely degraded (5x perplexity increase)!\n",
            "\n",
            "Sample generation:\n",
            "who said truth justice and the american way. The way.\n",
            "The way.\n",
            "Previous way.\n",
            "Previous way. The way.\n",
            "Previous way.\n",
            "Previous way. The way.\n",
            "Previous way. The way.\n",
            "Previous way. The way. The way.\n",
            "Previous...\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT SUMMARY\n",
            "============================================================\n",
            "\n",
            "BASELINE:\n",
            "  Params: 8,030,261,248\n",
            "  Perplexity: 15.46\n",
            "  Throughput: 1378.58 tok/s\n",
            "\n",
            "MLP25_ATTN20:\n",
            "  Params: 21.7% reduction\n",
            "  Perplexity: +704.0% change\n",
            "  Throughput: +6.5% change\n",
            "\n",
            "MLP40_ATTN30:\n",
            "  Params: 35.1% reduction\n",
            "  Perplexity: +4532.3% change\n",
            "  Throughput: +7.1% change\n",
            "\n",
            "MLP60_ATTN50:\n",
            "  Params: 51.0% reduction\n",
            "  Perplexity: +20833.0% change\n",
            "  Throughput: +7.3% change\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "class LlamaFFNPruner:\n",
        "    def __init__(self, model_id=\"meta-llama/Llama-3.1-8B-Instruct\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = 'left'\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            # attn_implementation=\"flash_attention_2\",\n",
        "            # attn_implementation=\"sdpa\",\n",
        "            # attn_implementation=\"eager\",\n",
        "            device_map={\"\": \"cuda\"}\n",
        "        )\n",
        "        print(f\"Attention implementation: {self.model.config._attn_implementation}\")  # ← ADD THIS LINE\n",
        "\n",
        "\n",
        "        self.original_intermediate_size = self.model.config.intermediate_size\n",
        "        self.original_num_heads = self.model.config.num_attention_heads\n",
        "        self.original_num_kv_heads = self.model.config.num_key_value_heads\n",
        "\n",
        "        print(f\"Original intermediate size: {self.original_intermediate_size}\")\n",
        "        print(f\"Original attention heads: {self.original_num_heads}\")\n",
        "        print(f\"Original KV heads: {self.original_num_kv_heads}\")\n",
        "\n",
        "    def compute_neuron_pair_importance(self, gate_weight, up_weight):\n",
        "        gate_max_abs = torch.max(torch.abs(gate_weight), dim=1).values\n",
        "        up_max_abs = torch.max(torch.abs(up_weight), dim=1).values\n",
        "        importance_scores = gate_max_abs + up_max_abs\n",
        "        return importance_scores\n",
        "\n",
        "    def compute_attention_head_importance(self, layer):\n",
        "        \"\"\"\n",
        "        Compute importance scores for attention heads using output projection weights.\n",
        "        Higher L2 norm = more important head.\n",
        "        \"\"\"\n",
        "        attn = layer.self_attn\n",
        "        o_proj_weight = attn.o_proj.weight.data\n",
        "\n",
        "        # Fix: Use config instead of attn attributes\n",
        "        num_heads = self.model.config.num_attention_heads\n",
        "        head_dim = self.model.config.hidden_size // num_heads\n",
        "\n",
        "        head_importance = []\n",
        "        for head_idx in range(num_heads):\n",
        "            start_idx = head_idx * head_dim\n",
        "            end_idx = start_idx + head_dim\n",
        "            head_weights = o_proj_weight[:, start_idx:end_idx]\n",
        "            importance = torch.norm(head_weights, p=2).item()\n",
        "            head_importance.append(importance)\n",
        "\n",
        "        return torch.tensor(head_importance)\n",
        "\n",
        "    def prune_attention_heads(self, layer, prune_percent):\n",
        "        \"\"\"\n",
        "        Prune attention heads in a layer.\n",
        "        Handles Grouped Query Attention (GQA) correctly.\n",
        "        CRITICAL: Maintains proper Q/KV head ratio for GQA.\n",
        "        \"\"\"\n",
        "        attn = layer.self_attn\n",
        "\n",
        "        # Use config attributes\n",
        "        num_heads = self.model.config.num_attention_heads\n",
        "        head_dim = self.model.config.hidden_size // num_heads\n",
        "        hidden_size = self.model.config.hidden_size\n",
        "        num_key_value_heads = self.model.config.num_key_value_heads\n",
        "\n",
        "        # Calculate group size (how many Q heads share one KV head)\n",
        "        num_groups = num_heads // num_key_value_heads\n",
        "\n",
        "        # Compute importance and select heads to keep\n",
        "        head_importance = self.compute_attention_head_importance(layer)\n",
        "        num_to_prune = int(prune_percent * num_heads)\n",
        "        num_heads_to_keep = num_heads - num_to_prune\n",
        "\n",
        "        if num_heads_to_keep <= 0:\n",
        "            raise ValueError(f\"Pruning {prune_percent*100}% would remove all attention heads!\")\n",
        "\n",
        "        # CRITICAL FIX: Ensure kept heads are divisible by num_groups\n",
        "        # This maintains the GQA structure\n",
        "        num_heads_to_keep = (num_heads_to_keep // num_groups) * num_groups\n",
        "\n",
        "        if num_heads_to_keep == 0:\n",
        "            # If rounding down gives 0, keep at least one group\n",
        "            num_heads_to_keep = num_groups\n",
        "\n",
        "        _, indices_to_keep = torch.topk(head_importance, num_heads_to_keep, largest=True, sorted=True)\n",
        "        indices_to_keep = indices_to_keep.sort().values.tolist()\n",
        "\n",
        "        # CRITICAL FIX: Select complete groups of heads\n",
        "        # Group heads by their KV head assignment\n",
        "        kv_head_groups = {}\n",
        "        for q_head_idx in range(num_heads):\n",
        "            kv_head_idx = q_head_idx // num_groups\n",
        "            if kv_head_idx not in kv_head_groups:\n",
        "                kv_head_groups[kv_head_idx] = []\n",
        "            kv_head_groups[kv_head_idx].append(q_head_idx)\n",
        "\n",
        "        # Calculate group importance (sum of all Q heads in group)\n",
        "        group_importance = []\n",
        "        for kv_idx in sorted(kv_head_groups.keys()):\n",
        "            group_heads = kv_head_groups[kv_idx]\n",
        "            group_imp = sum(head_importance[h].item() for h in group_heads)\n",
        "            group_importance.append((group_imp, kv_idx))\n",
        "\n",
        "        # Select top groups to keep\n",
        "        group_importance.sort(reverse=True)\n",
        "        num_kv_heads_to_keep = num_heads_to_keep // num_groups\n",
        "        kv_heads_to_keep = [kv_idx for _, kv_idx in group_importance[:num_kv_heads_to_keep]]\n",
        "        kv_heads_to_keep.sort()\n",
        "\n",
        "        # Get all Q heads that belong to kept KV heads\n",
        "        indices_to_keep = []\n",
        "        for kv_idx in kv_heads_to_keep:\n",
        "            indices_to_keep.extend(kv_head_groups[kv_idx])\n",
        "        indices_to_keep.sort()\n",
        "\n",
        "        # Calculate new dimensions\n",
        "        new_num_heads = len(indices_to_keep)\n",
        "        new_num_kv_heads = len(kv_heads_to_keep)\n",
        "        new_hidden_size = new_num_heads * head_dim\n",
        "        new_kv_hidden_size = new_num_kv_heads * head_dim\n",
        "\n",
        "        # Verify GQA constraint\n",
        "        assert new_num_heads % new_num_kv_heads == 0, \\\n",
        "            f\"Q heads ({new_num_heads}) must be divisible by KV heads ({new_num_kv_heads})\"\n",
        "\n",
        "        # --- Prune Q projection ---\n",
        "        q_indices = []\n",
        "        for head_idx in indices_to_keep:\n",
        "            start = head_idx * head_dim\n",
        "            q_indices.extend(range(start, start + head_dim))\n",
        "\n",
        "        new_q_proj = nn.Linear(hidden_size, new_hidden_size, bias=False).to(self.device)\n",
        "        new_q_proj.weight.data = attn.q_proj.weight.data[q_indices, :]\n",
        "\n",
        "        # --- Prune K projection ---\n",
        "        k_indices = []\n",
        "        for kv_head_idx in kv_heads_to_keep:\n",
        "            start = kv_head_idx * head_dim\n",
        "            k_indices.extend(range(start, start + head_dim))\n",
        "\n",
        "        new_k_proj = nn.Linear(hidden_size, new_kv_hidden_size, bias=False).to(self.device)\n",
        "        new_k_proj.weight.data = attn.k_proj.weight.data[k_indices, :]\n",
        "\n",
        "        # --- Prune V projection ---\n",
        "        new_v_proj = nn.Linear(hidden_size, new_kv_hidden_size, bias=False).to(self.device)\n",
        "        new_v_proj.weight.data = attn.v_proj.weight.data[k_indices, :]\n",
        "\n",
        "        # --- Prune O projection ---\n",
        "        new_o_proj = nn.Linear(new_hidden_size, hidden_size, bias=False).to(self.device)\n",
        "        new_o_proj.weight.data = attn.o_proj.weight.data[:, q_indices]\n",
        "\n",
        "        # Replace projections\n",
        "        attn.q_proj = new_q_proj\n",
        "        attn.k_proj = new_k_proj\n",
        "        attn.v_proj = new_v_proj\n",
        "        attn.o_proj = new_o_proj\n",
        "\n",
        "        return new_num_heads, new_num_kv_heads\n",
        "\n",
        "    def prune_mlp_layer(self, mlp, prune_percent):\n",
        "        gate_weight = mlp.gate_proj.weight.data.float()\n",
        "        up_weight = mlp.up_proj.weight.data.float()\n",
        "        importance_scores = self.compute_neuron_pair_importance(gate_weight, up_weight)\n",
        "\n",
        "        original_size = gate_weight.size(0)\n",
        "        num_to_prune = int(prune_percent * original_size)\n",
        "        k = original_size - num_to_prune\n",
        "        ALIGNMENT = 256\n",
        "        k = (k // ALIGNMENT) * ALIGNMENT\n",
        "        if k == 0:\n",
        "            k = ALIGNMENT\n",
        "        print(f\"    Aligning to {k} neurons (was {original_size - num_to_prune})\")\n",
        "\n",
        "        if k <= 0:\n",
        "            raise ValueError(f\"Pruning {prune_percent*100}% would remove all neurons!\")\n",
        "\n",
        "        _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
        "        indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "        new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(self.device)\n",
        "        new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(self.device)\n",
        "        new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(self.device)\n",
        "\n",
        "        new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
        "        new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
        "        new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
        "\n",
        "        return new_gate_proj, new_up_proj, new_down_proj, k\n",
        "\n",
        "    def prune_model(self, mlp_prune_percent, attention_prune_percent):\n",
        "        \"\"\"\n",
        "        Prune both MLP and attention layers.\n",
        "\n",
        "        Args:\n",
        "            mlp_prune_percent: Fraction of MLP neurons to remove (e.g., 0.4 = remove 40%)\n",
        "            attention_prune_percent: Fraction of attention heads to remove (e.g., 0.3 = remove 30%)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Pruning Model: MLP={mlp_prune_percent*100:.0f}%, Attention={attention_prune_percent*100:.0f}%\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        new_intermediate_size = None\n",
        "        new_num_heads = None\n",
        "        new_num_kv_heads = None\n",
        "\n",
        "        total_mlp_neurons_original = 0\n",
        "        total_mlp_neurons_kept = 0\n",
        "        total_attention_heads_original = 0\n",
        "        total_attention_heads_kept = 0\n",
        "\n",
        "        for idx, layer in enumerate(self.model.model.layers):\n",
        "            # --- Prune MLP ---\n",
        "            mlp = layer.mlp\n",
        "            new_gate, new_up, new_down, mlp_size = self.prune_mlp_layer(mlp, mlp_prune_percent)\n",
        "            mlp.gate_proj = new_gate\n",
        "            mlp.up_proj = new_up\n",
        "            mlp.down_proj = new_down\n",
        "\n",
        "            total_mlp_neurons_original += self.original_intermediate_size\n",
        "            total_mlp_neurons_kept += mlp_size\n",
        "\n",
        "            # --- Prune Attention ---\n",
        "            attn_heads, kv_heads = self.prune_attention_heads(layer, attention_prune_percent)\n",
        "\n",
        "            total_attention_heads_original += self.original_num_heads\n",
        "            total_attention_heads_kept += attn_heads\n",
        "\n",
        "            if new_intermediate_size is None:\n",
        "                new_intermediate_size = mlp_size\n",
        "                new_num_heads = attn_heads\n",
        "                new_num_kv_heads = kv_heads\n",
        "                print(f\"Layer 0:\")\n",
        "                print(f\"  MLP: {self.original_intermediate_size} -> {mlp_size} neurons\")\n",
        "                print(f\"  Attention: {self.original_num_heads} -> {attn_heads} heads (KV: {self.original_num_kv_heads} -> {kv_heads})\")\n",
        "\n",
        "            if (idx + 1) % 8 == 0:\n",
        "                print(f\"Processed {idx + 1}/{len(self.model.model.layers)} layers...\")\n",
        "\n",
        "        # Update model config\n",
        "        self.model.config.intermediate_size = new_intermediate_size\n",
        "        self.model.config.num_attention_heads = new_num_heads\n",
        "        self.model.config.num_key_value_heads = new_num_kv_heads\n",
        "\n",
        "        # Print summary\n",
        "        mlp_kept_pct = (total_mlp_neurons_kept / total_mlp_neurons_original) * 100\n",
        "        attn_kept_pct = (total_attention_heads_kept / total_attention_heads_original) * 100\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Pruning Complete!\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"MLP Neurons:      {total_mlp_neurons_kept:,}/{total_mlp_neurons_original:,} ({mlp_kept_pct:.1f}% kept)\")\n",
        "        print(f\"Attention Heads:  {total_attention_heads_kept}/{total_attention_heads_original} ({attn_kept_pct:.1f}% kept)\")\n",
        "        print(f\"Final MLP size:   {new_intermediate_size}\")\n",
        "        print(f\"Final Attention:  {new_num_heads} heads, {new_num_kv_heads} KV heads\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def get_model_size_mb(self):\n",
        "        param_size = sum(p.numel() * p.element_size() for p in self.model.parameters())\n",
        "        return param_size / (1024 * 1024)\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def evaluate_perplexity(self, dataset_name=\"wikitext\", split=\"test\", max_samples=100):\n",
        "        \"\"\"FIXED: Correct perplexity calculation\"\"\"\n",
        "        print(f\"\\nEvaluating perplexity on {dataset_name}...\")\n",
        "\n",
        "        if dataset_name == \"wikitext\":\n",
        "            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
        "        else:\n",
        "            dataset = load_dataset(dataset_name, split=split)\n",
        "\n",
        "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for item in tqdm(dataset, desc=\"Computing perplexity\"):\n",
        "                text = item['text'] if 'text' in item else str(item)\n",
        "\n",
        "                if len(text.strip()) == 0:\n",
        "                    continue\n",
        "\n",
        "                encodings = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "                input_ids = encodings.input_ids.to(self.device)\n",
        "\n",
        "                if input_ids.shape[1] < 2:\n",
        "                    continue\n",
        "\n",
        "                outputs = self.model(input_ids, labels=input_ids)\n",
        "                num_tokens = input_ids.shape[1]\n",
        "                total_loss += outputs.loss.item() * num_tokens\n",
        "                total_tokens += num_tokens\n",
        "\n",
        "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "        perplexity = np.exp(avg_loss)\n",
        "        return perplexity\n",
        "\n",
        "    def evaluate_generation_quality(self, prompts, max_new_tokens=300, batch_size=64):\n",
        "        \"\"\"FIXED: Correct throughput calculation\"\"\"\n",
        "        print(f\"\\nEvaluating generation quality on {len(prompts)} prompts (batch_size={batch_size})...\")\n",
        "\n",
        "        all_outputs = []\n",
        "        total_time = 0\n",
        "        total_tokens = 0\n",
        "        num_prompts = 0\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating batches\"):\n",
        "            batch_prompts = prompts[i:i+batch_size]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                batch_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            start_time = time.time()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    min_new_tokens=max_new_tokens,\n",
        "                    do_sample=False,\n",
        "                    # temperature=0.7,\n",
        "                    # top_p=0.9,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            batch_time = time.time() - start_time\n",
        "\n",
        "            # Decode outputs\n",
        "            for j in range(len(batch_prompts)):\n",
        "                generated_text = self.tokenizer.decode(outputs[j], skip_special_tokens=True)\n",
        "                all_outputs.append(generated_text)\n",
        "\n",
        "            # Count tokens based on MINIMUM guaranteed length\n",
        "            batch_tokens = len(batch_prompts) * max_new_tokens\n",
        "\n",
        "            total_time += batch_time\n",
        "            num_prompts += len(batch_prompts)\n",
        "\n",
        "        # Calculate throughput based on guaranteed minimum tokens\n",
        "        total_tokens = num_prompts * max_new_tokens\n",
        "        throughput = total_tokens / total_time if total_time > 0 else 0\n",
        "\n",
        "        results = {\n",
        "            'outputs': all_outputs,\n",
        "            'total_time': total_time,\n",
        "            'total_tokens': total_tokens,\n",
        "            'tokens_per_second': throughput,\n",
        "            'avg_time_per_prompt': total_time / len(prompts),\n",
        "        }\n",
        "\n",
        "        print(f\"Total tokens: {total_tokens}, Total time: {total_time:.2f}s, Throughput: {throughput:.2f} tok/s\")\n",
        "        return results\n",
        "\n",
        "    def measure_inference_speed(self, batch_sizes=[16], seq_length=300, num_runs=1):\n",
        "        print(f\"\\nMeasuring inference speed...\")\n",
        "        speed_results = {}\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_size in batch_sizes:\n",
        "            input_ids = torch.randint(0, self.tokenizer.vocab_size, (batch_size, seq_length)).to(self.device)\n",
        "\n",
        "            # Warmup\n",
        "            with torch.no_grad():\n",
        "                for _ in range(3):\n",
        "                    _ = self.model(input_ids)\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            times = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(num_runs):\n",
        "                    start = time.time()\n",
        "                    _ = self.model(input_ids)\n",
        "                    torch.cuda.synchronize()\n",
        "                    times.append(time.time() - start)\n",
        "\n",
        "            avg_time = np.mean(times)\n",
        "            throughput = (batch_size * seq_length) / avg_time\n",
        "\n",
        "            speed_results[batch_size] = {\n",
        "                'avg_time_sec': avg_time,\n",
        "                'throughput_tokens_per_sec': throughput\n",
        "            }\n",
        "\n",
        "            print(f\"Batch {batch_size}: {avg_time:.4f}s, {throughput:.2f} tokens/sec\")\n",
        "\n",
        "        return speed_results\n",
        "\n",
        "\n",
        "def run_pruning_experiment(prune_configs):\n",
        "    \"\"\"\n",
        "    Run pruning experiments with different MLP and attention pruning percentages.\n",
        "\n",
        "    Args:\n",
        "        prune_configs: List of tuples (mlp_prune_pct, attention_prune_pct)\n",
        "                      Example: [(0.4, 0.3), (0.6, 0.5)]\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    train_data_sample = load_from_disk(\"/content/drive/MyDrive/nq_subset\")\n",
        "    test_prompts = random.sample(train_data_sample[\"question\"][\"text\"], 64)\n",
        "\n",
        "    # Baseline evaluation (first config)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BASELINE (No Pruning)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    pruner = LlamaFFNPruner()\n",
        "    evaluator = ModelEvaluator(pruner.model, pruner.tokenizer, pruner.device)\n",
        "\n",
        "    baseline_params = pruner.count_parameters()\n",
        "    baseline_ppl = evaluator.evaluate_perplexity(max_samples=50)\n",
        "    baseline_gen = evaluator.evaluate_generation_quality(test_prompts)\n",
        "\n",
        "    results['baseline'] = {\n",
        "        'params': baseline_params,\n",
        "        'perplexity': baseline_ppl,\n",
        "        'tokens_per_second': baseline_gen['tokens_per_second'],\n",
        "        'total_tokens': baseline_gen['total_tokens'],\n",
        "        'total_time': baseline_gen['total_time'],\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- BASELINE RESULTS ---\")\n",
        "    print(f\"Parameters: {baseline_params:,}\")\n",
        "    print(f\"Perplexity: {baseline_ppl:.2f}\")\n",
        "    print(f\"Throughput: {baseline_gen['tokens_per_second']:.2f} tok/s\")\n",
        "\n",
        "    del pruner\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Run pruning experiments\n",
        "    for mlp_prune_pct, attn_prune_pct in prune_configs:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PRUNING EXPERIMENT: MLP={mlp_prune_pct*100:.0f}%, Attention={attn_prune_pct*100:.0f}%\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        pruner = LlamaFFNPruner()\n",
        "\n",
        "        pruner.prune_model(mlp_prune_pct, attn_prune_pct)\n",
        "        evaluator = ModelEvaluator(pruner.model, pruner.tokenizer, pruner.device)\n",
        "\n",
        "        pruned_params = pruner.count_parameters()\n",
        "        pruned_ppl = evaluator.evaluate_perplexity(max_samples=50)\n",
        "        pruned_gen = evaluator.evaluate_generation_quality(test_prompts)\n",
        "\n",
        "        param_reduction = (1 - pruned_params / baseline_params) * 100\n",
        "        ppl_degradation = ((pruned_ppl - baseline_ppl) / baseline_ppl) * 100\n",
        "        speed_change = ((pruned_gen['tokens_per_second'] - baseline_gen['tokens_per_second']) /\n",
        "                       baseline_gen['tokens_per_second']) * 100\n",
        "\n",
        "        print(f\"\\n--- RESULTS ---\")\n",
        "        print(f\"Parameters: {param_reduction:.1f}% reduction ({pruned_params:,} params)\")\n",
        "        print(f\"Perplexity: {pruned_ppl:.2f} ({ppl_degradation:+.1f}% change)\")\n",
        "        print(f\"Throughput: {pruned_gen['tokens_per_second']:.2f} tok/s ({speed_change:+.1f}%)\")\n",
        "\n",
        "        if pruned_ppl > 5 * baseline_ppl:\n",
        "            print(\"\\n⚠️  WARNING: Model severely degraded (5x perplexity increase)!\")\n",
        "\n",
        "        print(f\"\\nSample generation:\")\n",
        "        print(f\"{pruned_gen['outputs'][0][:200]}...\")\n",
        "\n",
        "        config_key = f\"mlp{int(mlp_prune_pct*100)}_attn{int(attn_prune_pct*100)}\"\n",
        "        results[config_key] = {\n",
        "            'mlp_prune_pct': mlp_prune_pct,\n",
        "            'attn_prune_pct': attn_prune_pct,\n",
        "            'params': pruned_params,\n",
        "            'param_reduction_pct': param_reduction,\n",
        "            'perplexity': pruned_ppl,\n",
        "            'ppl_degradation_pct': ppl_degradation,\n",
        "            'tokens_per_second': pruned_gen['tokens_per_second'],\n",
        "            'speed_change_pct': speed_change,\n",
        "            'total_tokens': pruned_gen['total_tokens'],\n",
        "            'total_time': pruned_gen['total_time'],\n",
        "        }\n",
        "\n",
        "        del pruner\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # login(token=os.getenv(\"HF_TOKEN\", \"YOUR_HUGGINGFACE_TOKEN_HERE\"))\n",
        "\n",
        "\n",
        "    # Define pruning configurations to test\n",
        "    # Format: (mlp_prune_percent, attention_prune_percent)\n",
        "    prune_configs = [\n",
        "        (0.25, 0.20),\n",
        "        (0.4, 0.3),   # Moderate: 40% MLP, 30% attention\n",
        "        (0.6, 0.5),   # Aggressive: 60% MLP, 50% attention\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Alternative: Test only MLP vs MLP+Attention\n",
        "    # prune_configs = [\n",
        "    #     (0.6, 0.0),   # 60% MLP only (no attention pruning)\n",
        "    #     (0.6, 0.5),   # 60% MLP + 50% attention\n",
        "    # ]\n",
        "\n",
        "    results = run_pruning_experiment(prune_configs)\n",
        "\n",
        "    # # Save results\n",
        "    # with open('pruning_results.json', 'w') as f:\n",
        "    #     json.dump({k: {kk: vv for kk, vv in v.items() if kk != 'outputs'}\n",
        "    #                for k, v in results.items()}, f, indent=2)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EXPERIMENT SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    for config, result in results.items():\n",
        "        if config == 'baseline':\n",
        "            print(f\"\\n{config.upper()}:\")\n",
        "            print(f\"  Params: {result['params']:,}\")\n",
        "            print(f\"  Perplexity: {result['perplexity']:.2f}\")\n",
        "            print(f\"  Throughput: {result['tokens_per_second']:.2f} tok/s\")\n",
        "        else:\n",
        "            print(f\"\\n{config.upper()}:\")\n",
        "            print(f\"  Params: {result['param_reduction_pct']:.1f}% reduction\")\n",
        "            print(f\"  Perplexity: {result['ppl_degradation_pct']:+.1f}% change\")\n",
        "            print(f\"  Throughput: {result['speed_change_pct']:+.1f}% change\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3c4ea538ee3f499ba478253b0d1e667f",
            "a504a18dbbb5445a8756dd3b67451f70",
            "a28914d1b8a14b7ba8bc4e81eed6cae8",
            "34996de2d4d04c60a9f29cfaf33f4b49",
            "c6495124057b4d6ea522a10aac3a44d2",
            "46041f75f7254fcda85cef2b0401d165",
            "9268369c051548d598f7f884c95fcb9a",
            "8305a78a8bf9432d8f7ac01795abea54",
            "00e9d9fd04da4c1f873816558bbe79ae",
            "967d3ee39bc245b7a65c96594e5f38e5",
            "8b8bb891412c420a980f9e41281e13c9"
          ]
        },
        "id": "x1QmughDzUr2",
        "outputId": "6a988a54-f759-4527-c4cd-2c43b1f6148f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c4ea538ee3f499ba478253b0d1e667f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original intermediate size: 14336\n",
            "Original attention heads: 32\n",
            "Original KV heads: 8\n",
            "\n",
            "======================================================================\n",
            "Memory Bandwidth Analysis: BASELINE\n",
            "======================================================================\n",
            "\n",
            "Total Parameters: 8,030,261,248\n",
            "Total Memory: 15316.51 MB (14.958 GB)\n",
            "\n",
            "Breakdown:\n",
            "  Attention: 1,342,177,280 params = 2560.00 MB\n",
            "  MLP:       5,637,144,576 params = 10752.00 MB\n",
            "  Other:     2004.51 MB\n",
            "\n",
            "Memory per forward pass (single token generation):\n",
            "  All weights must be loaded: 15316.51 MB\n",
            "  32 layers × forward = 15316.51 MB total bandwidth\n",
            "\n",
            "======================================================================\n",
            "Pruning Model: MLP=60%, Attention=50%\n",
            "======================================================================\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Layer 0:\n",
            "  MLP: 14336 -> 5632 neurons\n",
            "  Attention: 32 -> 16 heads (KV: 8 -> 4)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Processed 8/32 layers...\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Processed 16/32 layers...\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Processed 24/32 layers...\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "    Aligning to 5632 neurons (was 5735)\n",
            "Processed 32/32 layers...\n",
            "\n",
            "======================================================================\n",
            "Pruning Complete!\n",
            "======================================================================\n",
            "MLP Neurons:      180,224/458,752 (39.3% kept)\n",
            "Attention Heads:  512/1024 (50.0% kept)\n",
            "Final MLP size:   5632\n",
            "Final Attention:  16 heads, 4 KV heads\n",
            "\n",
            "======================================================================\n",
            "Memory Bandwidth Analysis: PRUNED\n",
            "======================================================================\n",
            "\n",
            "Total Parameters: 3,936,620,544\n",
            "Total Memory: 7508.51 MB (7.333 GB)\n",
            "\n",
            "Breakdown:\n",
            "  Attention: 671,088,640 params = 1280.00 MB\n",
            "  MLP:       2,214,592,512 params = 4224.00 MB\n",
            "  Other:     2004.51 MB\n",
            "\n",
            "Memory per forward pass (single token generation):\n",
            "  All weights must be loaded: 7508.51 MB\n",
            "  32 layers × forward = 7508.51 MB total bandwidth\n",
            "\n",
            "======================================================================\n",
            "MEMORY BANDWIDTH REDUCTION\n",
            "======================================================================\n",
            "Total: 15316.51 MB → 7508.51 MB\n",
            "Reduction: 51.0%\n",
            "\n",
            "Attention: 50.0% reduction\n",
            "MLP:       60.7% reduction\n"
          ]
        }
      ],
      "source": [
        "def calculate_memory_bandwidth_per_forward_pass(model, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Calculate total memory that needs to be loaded for one forward pass.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Memory Bandwidth Analysis: {model_name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    total_params = 0\n",
        "    total_bytes = 0\n",
        "\n",
        "    # Breakdown by layer type\n",
        "    attention_params = 0\n",
        "    attention_bytes = 0\n",
        "    mlp_params = 0\n",
        "    mlp_bytes = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "        num_bytes = param.numel() * param.element_size()\n",
        "\n",
        "        total_params += num_params\n",
        "        total_bytes += num_bytes\n",
        "\n",
        "        if 'self_attn' in name:\n",
        "            attention_params += num_params\n",
        "            attention_bytes += num_bytes\n",
        "        elif 'mlp' in name:\n",
        "            mlp_params += num_params\n",
        "            mlp_bytes += num_bytes\n",
        "\n",
        "    # Convert to readable units\n",
        "    total_mb = total_bytes / (1024**2)\n",
        "    total_gb = total_bytes / (1024**3)\n",
        "    attention_mb = attention_bytes / (1024**2)\n",
        "    mlp_mb = mlp_bytes / (1024**2)\n",
        "\n",
        "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
        "    print(f\"Total Memory: {total_mb:.2f} MB ({total_gb:.3f} GB)\")\n",
        "    print(f\"\\nBreakdown:\")\n",
        "    print(f\"  Attention: {attention_params:,} params = {attention_mb:.2f} MB\")\n",
        "    print(f\"  MLP:       {mlp_params:,} params = {mlp_mb:.2f} MB\")\n",
        "    print(f\"  Other:     {(total_bytes - attention_bytes - mlp_bytes)/(1024**2):.2f} MB\")\n",
        "\n",
        "    print(f\"\\nMemory per forward pass (single token generation):\")\n",
        "    print(f\"  All weights must be loaded: {total_mb:.2f} MB\")\n",
        "    print(f\"  32 layers × forward = {total_mb:.2f} MB total bandwidth\")\n",
        "\n",
        "    return {\n",
        "        'total_params': total_params,\n",
        "        'total_bytes': total_bytes,\n",
        "        'total_mb': total_mb,\n",
        "        'attention_mb': attention_mb,\n",
        "        'mlp_mb': mlp_mb,\n",
        "    }\n",
        "\n",
        "# Measure both models\n",
        "pruner = LlamaFFNPruner()\n",
        "baseline_mem = calculate_memory_bandwidth_per_forward_pass(pruner.model, \"BASELINE\")\n",
        "pruner.prune_model(0.6, 0.5)\n",
        "pruned_mem = calculate_memory_bandwidth_per_forward_pass(pruner.model, \"PRUNED\")\n",
        "\n",
        "# Calculate reduction\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"MEMORY BANDWIDTH REDUCTION\")\n",
        "print(f\"{'='*70}\")\n",
        "reduction_pct = (1 - pruned_mem['total_mb'] / baseline_mem['total_mb']) * 100\n",
        "print(f\"Total: {baseline_mem['total_mb']:.2f} MB → {pruned_mem['total_mb']:.2f} MB\")\n",
        "print(f\"Reduction: {reduction_pct:.1f}%\")\n",
        "\n",
        "attention_reduction = (1 - pruned_mem['attention_mb'] / baseline_mem['attention_mb']) * 100\n",
        "mlp_reduction = (1 - pruned_mem['mlp_mb'] / baseline_mem['mlp_mb']) * 100\n",
        "print(f\"\\nAttention: {attention_reduction:.1f}% reduction\")\n",
        "print(f\"MLP:       {mlp_reduction:.1f}% reduction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "Zq2hDy3wBIko",
        "outputId": "3176ae36-bdcf-4f00-d956-e7a34f0dac1c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_pruning'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1129696552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_pruning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_pruning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "import torch_pruning as tp\n",
        "import gc\n",
        "\n",
        "\n",
        "class LlamaFFNPruner:\n",
        "    def __init__(self, model_id=\"meta-llama/Llama-3.1-8B-Instruct\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = 'left'\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map={\"\": \"cuda\"}\n",
        "        )\n",
        "\n",
        "        print(f\"Model loaded: {model_id}\")\n",
        "        print(f\"Total parameters: {self.count_parameters():,}\")\n",
        "        self._print_memory_usage(\"After model load\")\n",
        "\n",
        "    def _print_memory_usage(self, stage=\"\"):\n",
        "        \"\"\"Print current GPU memory usage.\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "            reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "            print(f\"[{stage}] GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
        "\n",
        "    def _wrap_model_for_pruning(self):\n",
        "        \"\"\"\n",
        "        Wrap model to return only logits (not tuple) for torch-pruning compatibility.\n",
        "        \"\"\"\n",
        "        class ModelWrapper(nn.Module):\n",
        "            def __init__(self, model):\n",
        "                super().__init__()\n",
        "                self.model = model\n",
        "\n",
        "            def forward(self, input_ids):\n",
        "                # Get model output\n",
        "                outputs = self.model(input_ids)\n",
        "                # Return only logits (first element of output)\n",
        "                return outputs.logits\n",
        "\n",
        "        return ModelWrapper(self.model)\n",
        "\n",
        "    def prune_model(self, prune_ratio=0.5, round_to=256):\n",
        "        \"\"\"\n",
        "        Prune using torch-pruning MetaPruner with IN-PLACE weight replacement.\n",
        "\n",
        "        Args:\n",
        "            prune_ratio: Overall pruning ratio (0.5 = remove 50% of parameters)\n",
        "            round_to: Round pruned dimensions to multiples of this (256 for GPU efficiency)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Pruning Model with MetaPruner (IN-PLACE)\")\n",
        "        print(f\"Prune ratio: {prune_ratio*100:.0f}%\")\n",
        "        print(f\"GPU alignment: {round_to}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        self._print_memory_usage(\"Before pruning\")\n",
        "\n",
        "        # ⭐ Wrap model to return only logits (fix for tuple output issue)\n",
        "        wrapped_model = self._wrap_model_for_pruning()\n",
        "\n",
        "        # Create example inputs\n",
        "        example_inputs = torch.randint(0, 50000, (1, 128)).to(self.device)\n",
        "\n",
        "        # Identify layers to ignore (embeddings, layer norms)\n",
        "        ignored_layers = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, (nn.Embedding, nn.LayerNorm)):\n",
        "                ignored_layers.append(module)\n",
        "\n",
        "        print(f\"Ignoring {len(ignored_layers)} layers (embeddings, layer norms)\")\n",
        "\n",
        "        # Define importance metric (magnitude-based)\n",
        "        importance = tp.importance.MagnitudeImportance(p=2)\n",
        "\n",
        "        # Get baseline stats using wrapped model\n",
        "        print(\"\\nCalculating baseline FLOPs and parameters...\")\n",
        "        base_macs, base_params = tp.utils.count_ops_and_params(wrapped_model, example_inputs)\n",
        "        print(f\"Baseline: {base_params:,} parameters, {base_macs:,} MACs\")\n",
        "\n",
        "        # Create MetaPruner with wrapped model\n",
        "        print(\"\\nInitializing MetaPruner...\")\n",
        "        pruner = tp.pruner.MetaPruner(\n",
        "            wrapped_model,  # ⭐ Use wrapped model\n",
        "            example_inputs,\n",
        "            importance=importance,\n",
        "            pruning_ratio=prune_ratio,\n",
        "\n",
        "            # ⭐ Key parameters for GPU efficiency\n",
        "            round_to=round_to,  # Align all dimensions to multiples of 256\n",
        "            ignored_layers=ignored_layers,\n",
        "\n",
        "            # Pruning strategy\n",
        "            iterative_steps=1,  # One-shot pruning\n",
        "            global_pruning=False,  # Prune each layer independently\n",
        "            isomorphic=False,  # Allow different pruning ratios per layer\n",
        "        )\n",
        "\n",
        "        self._print_memory_usage(\"After pruner init\")\n",
        "\n",
        "        # Execute pruning (modifies wrapped_model.model in-place)\n",
        "        print(\"\\nExecuting one-shot pruning (in-place)...\")\n",
        "        pruner.step()\n",
        "        print(\"✓ Pruning complete!\")\n",
        "\n",
        "        # The original self.model is now pruned (it's inside wrapped_model.model)\n",
        "        # No need to reassign, it was modified in-place\n",
        "\n",
        "        # Clean up pruner and wrapper to free memory\n",
        "        del pruner\n",
        "        del wrapped_model\n",
        "        del example_inputs\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        self._print_memory_usage(\"After pruning and cleanup\")\n",
        "\n",
        "        # Get pruned stats (create new wrapper for verification)\n",
        "        print(\"\\nCalculating pruned FLOPs and parameters...\")\n",
        "        wrapped_model_check = self._wrap_model_for_pruning()\n",
        "        example_inputs_check = torch.randint(0, 50000, (1, 128)).to(self.device)\n",
        "        pruned_macs, pruned_params = tp.utils.count_ops_and_params(wrapped_model_check, example_inputs_check)\n",
        "        del wrapped_model_check\n",
        "        del example_inputs_check\n",
        "\n",
        "        # Print results\n",
        "        param_reduction = (1 - pruned_params / base_params) * 100\n",
        "        macs_reduction = (1 - pruned_macs / base_macs) * 100\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Pruning Results\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Parameters: {base_params:,} → {pruned_params:,} ({param_reduction:.1f}% reduction)\")\n",
        "        print(f\"MACs:       {base_macs:,} → {pruned_macs:,} ({macs_reduction:.1f}% reduction)\")\n",
        "\n",
        "        self._print_memory_usage(\"Final state\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "    def get_model_size_mb(self):\n",
        "        param_size = sum(p.numel() * p.element_size() for p in self.model.parameters())\n",
        "        return param_size / (1024 * 1024)\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def evaluate_perplexity(self, dataset_name=\"wikitext\", split=\"test\", max_samples=100):\n",
        "        \"\"\"Evaluate perplexity on a dataset.\"\"\"\n",
        "        print(f\"\\nEvaluating perplexity on {dataset_name}...\")\n",
        "\n",
        "        if dataset_name == \"wikitext\":\n",
        "            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
        "        else:\n",
        "            dataset = load_dataset(dataset_name, split=split)\n",
        "\n",
        "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for item in tqdm(dataset, desc=\"Computing perplexity\"):\n",
        "                text = item['text'] if 'text' in item else str(item)\n",
        "\n",
        "                if len(text.strip()) == 0:\n",
        "                    continue\n",
        "\n",
        "                encodings = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "                input_ids = encodings.input_ids.to(self.device)\n",
        "\n",
        "                if input_ids.shape[1] < 2:\n",
        "                    continue\n",
        "\n",
        "                outputs = self.model(input_ids, labels=input_ids)\n",
        "                num_tokens = input_ids.shape[1]\n",
        "                total_loss += outputs.loss.item() * num_tokens\n",
        "                total_tokens += num_tokens\n",
        "\n",
        "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
        "        perplexity = np.exp(avg_loss)\n",
        "        return perplexity\n",
        "\n",
        "    def evaluate_generation_quality(self, prompts, max_new_tokens=300, batch_size=32):\n",
        "        \"\"\"Evaluate generation throughput.\"\"\"\n",
        "        print(f\"\\nEvaluating generation quality on {len(prompts)} prompts (batch_size={batch_size})...\")\n",
        "\n",
        "        all_outputs = []\n",
        "        total_time = 0\n",
        "        num_prompts = 0\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating batches\"):\n",
        "            batch_prompts = prompts[i:i+batch_size]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                batch_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            ).to(self.device)\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            start_time = time.time()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    min_new_tokens=max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            batch_time = time.time() - start_time\n",
        "\n",
        "            # Decode outputs\n",
        "            for j in range(len(batch_prompts)):\n",
        "                generated_text = self.tokenizer.decode(outputs[j], skip_special_tokens=True)\n",
        "                all_outputs.append(generated_text)\n",
        "\n",
        "            total_time += batch_time\n",
        "            num_prompts += len(batch_prompts)\n",
        "\n",
        "        # Calculate throughput\n",
        "        total_tokens = num_prompts * max_new_tokens\n",
        "        throughput = total_tokens / total_time if total_time > 0 else 0\n",
        "\n",
        "        results = {\n",
        "            'outputs': all_outputs,\n",
        "            'total_time': total_time,\n",
        "            'total_tokens': total_tokens,\n",
        "            'tokens_per_second': throughput,\n",
        "            'avg_time_per_prompt': total_time / len(prompts),\n",
        "        }\n",
        "\n",
        "        print(f\"Total tokens: {total_tokens}, Total time: {total_time:.2f}s, Throughput: {throughput:.2f} tok/s\")\n",
        "        return results\n",
        "\n",
        "    def measure_forward_pass_speed(self, batch_size=32, seq_length=512, num_runs=1):\n",
        "        \"\"\"Measure forward pass throughput.\"\"\"\n",
        "        print(f\"\\nMeasuring forward pass speed (batch={batch_size}, seq={seq_length})...\")\n",
        "\n",
        "        input_ids = torch.randint(0, self.tokenizer.vocab_size, (batch_size, seq_length)).to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        # Warmup\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10):\n",
        "                _ = self.model(input_ids)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_runs):\n",
        "                _ = self.model(input_ids)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        throughput = (batch_size * seq_length * num_runs) / elapsed\n",
        "\n",
        "        print(f\"Batch {batch_size}, seq {seq_length}: {throughput:.2f} tok/s\")\n",
        "        return throughput\n",
        "\n",
        "\n",
        "def run_pruning_experiment(prune_ratios):\n",
        "    \"\"\"\n",
        "    Run pruning experiments with different pruning ratios.\n",
        "    Memory-efficient: only one model in memory at a time.\n",
        "\n",
        "    Args:\n",
        "        prune_ratios: List of pruning ratios to test\n",
        "                     Example: [0.3, 0.4, 0.5]\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Load test prompts\n",
        "    train_data_sample = load_from_disk(\"/content/drive/MyDrive/nq_subset\")\n",
        "    test_prompts = random.sample(train_data_sample[\"question\"][\"text\"], 64)\n",
        "\n",
        "    # ===== BASELINE EVALUATION =====\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"BASELINE (No Pruning)\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    baseline_pruner = LlamaFFNPruner()\n",
        "    baseline_evaluator = ModelEvaluator(\n",
        "        baseline_pruner.model,\n",
        "        baseline_pruner.tokenizer,\n",
        "        baseline_pruner.device\n",
        "    )\n",
        "\n",
        "    baseline_params = baseline_pruner.count_parameters()\n",
        "    baseline_ppl = baseline_evaluator.evaluate_perplexity(max_samples=50)\n",
        "    baseline_gen = baseline_evaluator.evaluate_generation_quality(test_prompts)\n",
        "    baseline_forward = baseline_evaluator.measure_forward_pass_speed()\n",
        "\n",
        "    results['baseline'] = {\n",
        "        'params': baseline_params,\n",
        "        'perplexity': baseline_ppl,\n",
        "        'generation_throughput': baseline_gen['tokens_per_second'],\n",
        "        'forward_pass_throughput': baseline_forward,\n",
        "        'total_tokens': baseline_gen['total_tokens'],\n",
        "        'total_time': baseline_gen['total_time'],\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"BASELINE RESULTS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Parameters:     {baseline_params:,}\")\n",
        "    print(f\"Perplexity:     {baseline_ppl:.2f}\")\n",
        "    print(f\"Generation:     {baseline_gen['tokens_per_second']:.2f} tok/s\")\n",
        "    print(f\"Forward pass:   {baseline_forward:.2f} tok/s\")\n",
        "\n",
        "    # ⭐ CRITICAL: Free baseline model memory before loading pruned models\n",
        "    del baseline_pruner\n",
        "    del baseline_evaluator\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"\\n✓ Baseline model freed from memory\")\n",
        "\n",
        "    # ===== PRUNING EXPERIMENTS =====\n",
        "    for prune_ratio in prune_ratios:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"PRUNING EXPERIMENT: {prune_ratio*100:.0f}% ratio\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Create and prune model (in-place, no extra memory)\n",
        "        pruner = LlamaFFNPruner()\n",
        "        pruner.prune_model(prune_ratio=prune_ratio, round_to=256)\n",
        "\n",
        "        # Evaluate pruned model\n",
        "        evaluator = ModelEvaluator(pruner.model, pruner.tokenizer, pruner.device)\n",
        "\n",
        "        pruned_params = pruner.count_parameters()\n",
        "        pruned_ppl = evaluator.evaluate_perplexity(max_samples=50)\n",
        "        pruned_gen = evaluator.evaluate_generation_quality(test_prompts)\n",
        "        pruned_forward = evaluator.measure_forward_pass_speed()\n",
        "\n",
        "        # Calculate metrics\n",
        "        param_reduction = (1 - pruned_params / baseline_params) * 100\n",
        "        ppl_degradation = ((pruned_ppl - baseline_ppl) / baseline_ppl) * 100\n",
        "        gen_speedup = pruned_gen['tokens_per_second'] / baseline_gen['tokens_per_second']\n",
        "        forward_speedup = pruned_forward / baseline_forward\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"RESULTS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Parameters:     {param_reduction:.1f}% reduction ({pruned_params:,} params)\")\n",
        "        print(f\"Perplexity:     {pruned_ppl:.2f} ({ppl_degradation:+.1f}% change)\")\n",
        "        print(f\"Generation:     {pruned_gen['tokens_per_second']:.2f} tok/s ({gen_speedup:.2f}x speedup)\")\n",
        "        print(f\"Forward pass:   {pruned_forward:.2f} tok/s ({forward_speedup:.2f}x speedup)\")\n",
        "\n",
        "        if pruned_ppl > 5 * baseline_ppl:\n",
        "            print(\"\\n⚠️  WARNING: Model severely degraded (5x perplexity increase)!\")\n",
        "\n",
        "        print(f\"\\nSample generation:\")\n",
        "        print(f\"{pruned_gen['outputs'][0][:200]}...\")\n",
        "\n",
        "        # Store results\n",
        "        config_key = f\"prune_{int(prune_ratio*100)}\"\n",
        "        results[config_key] = {\n",
        "            'prune_ratio': prune_ratio,\n",
        "            'params': pruned_params,\n",
        "            'param_reduction_pct': param_reduction,\n",
        "            'perplexity': pruned_ppl,\n",
        "            'ppl_degradation_pct': ppl_degradation,\n",
        "            'generation_throughput': pruned_gen['tokens_per_second'],\n",
        "            'generation_speedup': gen_speedup,\n",
        "            'forward_pass_throughput': pruned_forward,\n",
        "            'forward_speedup': forward_speedup,\n",
        "            'total_tokens': pruned_gen['total_tokens'],\n",
        "            'total_time': pruned_gen['total_time'],\n",
        "        }\n",
        "\n",
        "        # ⭐ CRITICAL: Free this pruned model before next iteration\n",
        "        del pruner\n",
        "        del evaluator\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"\\n✓ Pruned model {prune_ratio*100:.0f}% freed from memory\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import os\n",
        "    # Login with token from environment variable or set your token here\n",
        "    # login(token=os.getenv(\"HF_TOKEN\", \"YOUR_HUGGINGFACE_TOKEN_HERE\"))\n",
        "\n",
        "    # Define pruning ratios to test\n",
        "    prune_ratios = [\n",
        "        0.3,  # 30% pruning (conservative)\n",
        "        0.4,  # 40% pruning (moderate)\n",
        "        0.5,  # 50% pruning (aggressive)\n",
        "    ]\n",
        "\n",
        "    # Run experiments\n",
        "    results = run_pruning_experiment(prune_ratios)\n",
        "\n",
        "    # Save results\n",
        "    with open('pruning_results.json', 'w') as f:\n",
        "        json.dump({k: {kk: vv for kk, vv in v.items() if kk != 'outputs'}\n",
        "                   for k, v in results.items()}, f, indent=2)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"EXPERIMENT SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    for config, result in results.items():\n",
        "        if config == 'baseline':\n",
        "            print(f\"\\n{config.upper()}:\")\n",
        "            print(f\"  Params:      {result['params']:,}\")\n",
        "            print(f\"  Perplexity:  {result['perplexity']:.2f}\")\n",
        "            print(f\"  Generation:  {result['generation_throughput']:.2f} tok/s\")\n",
        "            print(f\"  Forward:     {result['forward_pass_throughput']:.2f} tok/s\")\n",
        "        else:\n",
        "            print(f\"\\n{config.upper()}:\")\n",
        "            print(f\"  Params:      {result['param_reduction_pct']:.1f}% reduction\")\n",
        "            print(f\"  Perplexity:  {result['ppl_degradation_pct']:+.1f}% change\")\n",
        "            print(f\"  Generation:  {result['generation_speedup']:.2f}x speedup\")\n",
        "            print(f\"  Forward:     {result['forward_speedup']:.2f}x speedup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "cb37b57587424f52b72d196708aa2be3",
            "a47fe9856c66409b9f81eaa0f4df3b97",
            "0515815885a640fc88ed164265749d85",
            "5a711718fffb4a39abc1b84fb655c792",
            "ef92c244f3ca48ee900dee53a137862a",
            "6c10be8fc8054c3bbf92c05f5dddc4ca",
            "8566d51277414019a2907df83d17389c",
            "de482c94baba4094934bb876f6d6590e",
            "12bdae9d1ce04e56882049f8df63a49d",
            "507cb23020ac4a849d05c78953f83434",
            "52b12e8723704691850cd29080f406d1"
          ]
        },
        "id": "1ToJF7LSV0Z2",
        "outputId": "2ae4c243-5db3-45f0-99b5-e912431549ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb37b57587424f52b72d196708aa2be3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3779285638.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Test baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mpruner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaFFNPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mbaseline_speed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_forward_pass_speed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2751404175.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_id)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_side\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         self.model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5046\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5047\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5048\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5049\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5050\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5468\u001b[0;31m                 \u001b[0m_error_msgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shard_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5469\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_error_msgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         disk_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mto_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def test_forward_pass_speed(model, batch_size=32, seq_len=512, num_runs=2):\n",
        "    \"\"\"\n",
        "    Test pure forward pass speed (no generation).\n",
        "    \"\"\"\n",
        "    input_ids = torch.randint(0, 50000, (batch_size, seq_len)).cuda()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(input_ids)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            _ = model(input_ids)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    throughput = (batch_size * seq_len * num_runs) / elapsed\n",
        "    print(f\"Batch {batch_size}, seq {seq_len}: {throughput:.2f} tok/s\")\n",
        "    return throughput\n",
        "\n",
        "# Test baseline\n",
        "pruner = LlamaFFNPruner()\n",
        "baseline_speed = test_forward_pass_speed(pruner.model)\n",
        "\n",
        "pruner.prune_model(0.6, 0.5)\n",
        "# Test pruned\n",
        "pruned_speed = test_forward_pass_speed(pruner.model)\n",
        "\n",
        "# Calculate speedup\n",
        "speedup = pruned_speed / baseline_speed\n",
        "print(f\"Forward pass speedup: {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9fcb8bb64c854024bcb2e41dfdcbbafe",
            "2cc9c1e85ec54d2b81b4d4c1c93ab20e",
            "25eebbbbed104f48acb8291929ac2b54",
            "fff3f28324ae4322a3e705038bee305f",
            "da1fb0b1072f4ad299bd522a4a0903b6",
            "dddc32d2978d4e22b411a3c09eae8ba4",
            "2323f64bb2d448caa35cde8cee346ac4",
            "60fdffd40ca34431b90c1ad3e68084cd",
            "9eb00f3630534de9b4a0a5106f060013",
            "95750c3367144c259dc4cbe545601bc5",
            "fbbb6ce1c12f4b739b87ebb4ed3a275a"
          ]
        },
        "id": "vMtRgiIfFvLT",
        "outputId": "50bfbd8e-ec92-4eec-975a-ed8214725502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Testing Llama 3.1 8B with LocalPruner\n",
            "======================================================================\n",
            "\n",
            "1. Loading model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fcb8bb64c854024bcb2e41dfdcbbafe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded\n",
            "  Parameters: 8,030,261,248\n",
            "  GPU Memory: 36.79GB\n",
            "\n",
            "2. Pruning MLP layers (50% pruning, layer-by-layer)...\n",
            "\n",
            "  Pruning layer 0...\n",
            "    ❌ LocalPruner failed for layer 0: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 0 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 1...\n",
            "    ❌ LocalPruner failed for layer 1: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 1 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 2...\n",
            "    ❌ LocalPruner failed for layer 2: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 2 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 3...\n",
            "    ❌ LocalPruner failed for layer 3: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 3 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 4...\n",
            "    ❌ LocalPruner failed for layer 4: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 4 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 5...\n",
            "    ❌ LocalPruner failed for layer 5: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 5 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 6...\n",
            "    ❌ LocalPruner failed for layer 6: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 6 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 7...\n",
            "    ❌ LocalPruner failed for layer 7: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 7 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 8...\n",
            "    ❌ LocalPruner failed for layer 8: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 8 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 9...\n",
            "    ❌ LocalPruner failed for layer 9: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 9 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 10...\n",
            "    ❌ LocalPruner failed for layer 10: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 10 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 11...\n",
            "    ❌ LocalPruner failed for layer 11: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 11 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 12...\n",
            "    ❌ LocalPruner failed for layer 12: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 12 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 13...\n",
            "    ❌ LocalPruner failed for layer 13: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 13 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 14...\n",
            "    ❌ LocalPruner failed for layer 14: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 14 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 15...\n",
            "    ❌ LocalPruner failed for layer 15: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 15 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 16...\n",
            "    ❌ LocalPruner failed for layer 16: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 16 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 17...\n",
            "    ❌ LocalPruner failed for layer 17: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 17 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 18...\n",
            "    ❌ LocalPruner failed for layer 18: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 18 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 19...\n",
            "    ❌ LocalPruner failed for layer 19: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 19 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 20...\n",
            "    ❌ LocalPruner failed for layer 20: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 20 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 21...\n",
            "    ❌ LocalPruner failed for layer 21: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 21 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 22...\n",
            "    ❌ LocalPruner failed for layer 22: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 22 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 23...\n",
            "    ❌ LocalPruner failed for layer 23: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 23 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 24...\n",
            "    ❌ LocalPruner failed for layer 24: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 24 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 25...\n",
            "    ❌ LocalPruner failed for layer 25: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 25 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 26...\n",
            "    ❌ LocalPruner failed for layer 26: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 26 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 27...\n",
            "    ❌ LocalPruner failed for layer 27: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 27 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 28...\n",
            "    ❌ LocalPruner failed for layer 28: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 28 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 29...\n",
            "    ❌ LocalPruner failed for layer 29: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 29 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 30...\n",
            "    ❌ LocalPruner failed for layer 30: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 30 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "  Pruning layer 31...\n",
            "    ❌ LocalPruner failed for layer 31: 'DependencyGraph' object has no attribute 'get_pruning_plan'\n",
            "    Falling back to manual pruning...\n",
            "    ✓ Layer 31 (manual): 14336 -> 7168 neurons\n",
            "\n",
            "✓ Pruning complete in 26.3s!\n",
            "  MLP neurons: 458,752 -> 229,376\n",
            "  Reduction: 50.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3. Verifying results...\n",
            "  Parameters before: 8,030,261,248\n",
            "  Parameters after:  5,211,688,960\n",
            "  Reduction: 35.1%\n",
            "  GPU Memory: 31.54GB\n",
            "\n",
            "4. Testing forward pass...\n",
            "✓ Forward pass works: torch.Size([1, 128, 128256])\n",
            "\n",
            "5. Testing text generation...\n",
            "✓ Generation works!\n",
            "  Prompt: The future of AI is\n",
            "  Generated: The future of AI is going to be dominated by robots. Robots are going to be dominated by computers. Computers are going to\n",
            "\n",
            "6. Measuring forward pass speed...\n",
            "✓ Throughput: 393231.32 tok/s\n",
            "\n",
            "======================================================================\n",
            "✅ Llama 3.1 8B LocalPruner SUCCESS!\n",
            "======================================================================\n",
            "Model pruned from 8,030,261,248 to 5,211,688,960 parameters\n",
            "Reduction: 35.1%\n",
            "Pruning time: 26.3s\n",
            "Forward throughput: 393231.32 tok/s\n"
          ]
        }
      ],
      "source": [
        "def test_llama_local_pruning():\n",
        "    \"\"\"\n",
        "    Prune Llama 3.1 8B using LocalPruner (layer-by-layer pruning).\n",
        "    Faster than MetaPruner because it builds dependency graphs per layer.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    import torch_pruning as tp\n",
        "    from huggingface_hub import login\n",
        "    import gc\n",
        "    import time\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"Testing Llama 3.1 8B with LocalPruner\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Login with token from environment variable or set your token here\n",
        "    import os\n",
        "    # login(token=os.getenv(\"HF_TOKEN\", \"YOUR_HUGGINGFACE_TOKEN_HERE\"))\n",
        "\n",
        "    # Load model\n",
        "    print(\"\\n1. Loading model...\")\n",
        "    model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"cuda\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    params_before = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"✓ Model loaded\")\n",
        "    print(f\"  Parameters: {params_before:,}\")\n",
        "\n",
        "    # Print memory\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "        print(f\"  GPU Memory: {allocated:.2f}GB\")\n",
        "\n",
        "    # Local pruning function\n",
        "    def prune_layer_local(layer, layer_idx, prune_ratio=0.5, round_to=256):\n",
        "        \"\"\"\n",
        "        Prune a single layer using torch-pruning's local dependency graph.\n",
        "        \"\"\"\n",
        "        print(f\"\\n  Pruning layer {layer_idx}...\")\n",
        "\n",
        "        mlp = layer.mlp\n",
        "\n",
        "        # Calculate importance scores manually\n",
        "        gate_weight = mlp.gate_proj.weight.data\n",
        "        up_weight = mlp.up_proj.weight.data\n",
        "\n",
        "        gate_importance = torch.max(torch.abs(gate_weight), dim=1).values\n",
        "        up_importance = torch.max(torch.abs(up_weight), dim=1).values\n",
        "        importance = gate_importance + up_importance\n",
        "\n",
        "        # Determine neurons to prune\n",
        "        original_size = gate_weight.shape[0]\n",
        "        num_to_prune = int(original_size * prune_ratio)\n",
        "        num_to_keep = original_size - num_to_prune\n",
        "\n",
        "        # Round to alignment\n",
        "        num_to_keep = (num_to_keep // round_to) * round_to\n",
        "        if num_to_keep == 0:\n",
        "            num_to_keep = round_to\n",
        "\n",
        "        num_to_prune = original_size - num_to_keep\n",
        "\n",
        "        if num_to_prune <= 0:\n",
        "            print(f\"    No pruning needed\")\n",
        "            return original_size, original_size\n",
        "\n",
        "        # Get indices of least important neurons to prune\n",
        "        _, pruning_indices = torch.topk(importance, num_to_prune, largest=False)\n",
        "        pruning_indices = pruning_indices.sort().values\n",
        "\n",
        "        try:\n",
        "            # Create example input for this layer\n",
        "            # Input to MLP is [batch, seq_len, hidden_size]\n",
        "            hidden_size = mlp.gate_proj.in_features\n",
        "            example_input = torch.randn(1, 10, hidden_size).to(gate_weight.device).to(gate_weight.dtype)\n",
        "\n",
        "            # Build dependency graph for this layer's MLP only\n",
        "            DG = tp.DependencyGraph()\n",
        "            DG.build_dependency(mlp, example_inputs=example_input)\n",
        "\n",
        "            # Create pruning plan for gate_proj\n",
        "            # This will automatically handle gate_proj, up_proj, and down_proj dependencies\n",
        "            pruning_plan = DG.get_pruning_plan(\n",
        "                mlp.gate_proj,\n",
        "                tp.prune_linear_out_channels,\n",
        "                idxs=pruning_indices.tolist()\n",
        "            )\n",
        "\n",
        "            # Execute pruning\n",
        "            pruning_plan.exec()\n",
        "\n",
        "            print(f\"    ✓ Layer {layer_idx}: {original_size} -> {num_to_keep} neurons\")\n",
        "            return original_size, num_to_keep\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ❌ LocalPruner failed for layer {layer_idx}: {e}\")\n",
        "            print(f\"    Falling back to manual pruning...\")\n",
        "\n",
        "            # Fallback to manual pruning if LocalPruner fails\n",
        "            _, indices_to_keep = torch.topk(importance, num_to_keep, largest=True)\n",
        "            indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "            # Create new layers\n",
        "            new_gate = nn.Linear(\n",
        "                mlp.gate_proj.in_features,\n",
        "                num_to_keep,\n",
        "                bias=False\n",
        "            ).to(gate_weight.device).to(gate_weight.dtype)\n",
        "\n",
        "            new_up = nn.Linear(\n",
        "                mlp.up_proj.in_features,\n",
        "                num_to_keep,\n",
        "                bias=False\n",
        "            ).to(up_weight.device).to(up_weight.dtype)\n",
        "\n",
        "            new_down = nn.Linear(\n",
        "                num_to_keep,\n",
        "                mlp.down_proj.out_features,\n",
        "                bias=False\n",
        "            ).to(mlp.down_proj.weight.device).to(mlp.down_proj.weight.dtype)\n",
        "\n",
        "            # Copy weights\n",
        "            new_gate.weight.data = gate_weight[indices_to_keep, :]\n",
        "            new_up.weight.data = up_weight[indices_to_keep, :]\n",
        "            new_down.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
        "\n",
        "            # Replace\n",
        "            layer.mlp.gate_proj = new_gate\n",
        "            layer.mlp.up_proj = new_up\n",
        "            layer.mlp.down_proj = new_down\n",
        "\n",
        "            print(f\"    ✓ Layer {layer_idx} (manual): {original_size} -> {num_to_keep} neurons\")\n",
        "            return original_size, num_to_keep\n",
        "\n",
        "    # Prune all layers\n",
        "    print(\"\\n2. Pruning MLP layers (50% pruning, layer-by-layer)...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_original = 0\n",
        "    total_kept = 0\n",
        "\n",
        "    for i, layer in enumerate(model.model.layers):\n",
        "        orig, kept = prune_layer_local(layer, i, prune_ratio=0.5, round_to=256)\n",
        "        total_original += orig\n",
        "        total_kept += kept\n",
        "\n",
        "        # Clean up after each layer\n",
        "        if i % 4 == 0:\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    prune_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n✓ Pruning complete in {prune_time:.1f}s!\")\n",
        "    print(f\"  MLP neurons: {total_original:,} -> {total_kept:,}\")\n",
        "    print(f\"  Reduction: {(1 - total_kept/total_original)*100:.1f}%\")\n",
        "\n",
        "    # Clean up\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Check results\n",
        "    print(\"\\n3. Verifying results...\")\n",
        "    params_after = sum(p.numel() for p in model.parameters())\n",
        "    reduction = (1 - params_after / params_before) * 100\n",
        "\n",
        "    print(f\"  Parameters before: {params_before:,}\")\n",
        "    print(f\"  Parameters after:  {params_after:,}\")\n",
        "    print(f\"  Reduction: {reduction:.1f}%\")\n",
        "\n",
        "    # Print memory\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "        print(f\"  GPU Memory: {allocated:.2f}GB\")\n",
        "\n",
        "    # Test forward pass\n",
        "    print(\"\\n4. Testing forward pass...\")\n",
        "    test_input = torch.randint(0, 50000, (1, 128)).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(test_input)\n",
        "\n",
        "    print(f\"✓ Forward pass works: {outputs.logits.shape}\")\n",
        "\n",
        "    # Test generation\n",
        "    print(\"\\n5. Testing text generation...\")\n",
        "    test_prompt = \"The future of AI is\"\n",
        "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=20,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"✓ Generation works!\")\n",
        "    print(f\"  Prompt: {test_prompt}\")\n",
        "    print(f\"  Generated: {generated_text}\")\n",
        "\n",
        "    # Measure speed\n",
        "    print(\"\\n6. Measuring forward pass speed...\")\n",
        "    batch_size = 32\n",
        "    seq_len = 512\n",
        "    test_input = torch.randint(0, 50000, (batch_size, seq_len)).cuda()\n",
        "\n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):\n",
        "            _ = model(test_input)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):\n",
        "            _ = model(test_input)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    throughput = (batch_size * seq_len * 100) / elapsed\n",
        "    print(f\"✓ Throughput: {throughput:.2f} tok/s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✅ Llama 3.1 8B LocalPruner SUCCESS!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Model pruned from {params_before:,} to {params_after:,} parameters\")\n",
        "    print(f\"Reduction: {reduction:.1f}%\")\n",
        "    print(f\"Pruning time: {prune_time:.1f}s\")\n",
        "    print(f\"Forward throughput: {throughput:.2f} tok/s\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# Run the test\n",
        "pruned_model, tokenizer = test_llama_local_pruning()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00e9d9fd04da4c1f873816558bbe79ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0515815885a640fc88ed164265749d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de482c94baba4094934bb876f6d6590e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12bdae9d1ce04e56882049f8df63a49d",
            "value": 1
          }
        },
        "0abe528b1a94430bb989d5c1d1edf29e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb1370d7c82457183d154800be61ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbf3b6e62a984e2485fa844fb4195dd0",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2aa8b2acec0942679b99134233f0e44f",
            "value": 4
          }
        },
        "0f8361d616204c70b2fea12c42642eda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12bdae9d1ce04e56882049f8df63a49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b80c0d79bf846ccbb1529fc074a2c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f8361d616204c70b2fea12c42642eda",
            "placeholder": "​",
            "style": "IPY_MODEL_5707d2163bb848f3b59ad8e6e5321fd5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1df50ac2bc704d1fb89edecf154ded2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b80c0d79bf846ccbb1529fc074a2c1a",
              "IPY_MODEL_eab3e0dd104d4f9396ffb684c7e8c811",
              "IPY_MODEL_97ef8e3cb10a42988472f302c386baf3"
            ],
            "layout": "IPY_MODEL_0abe528b1a94430bb989d5c1d1edf29e"
          }
        },
        "2323f64bb2d448caa35cde8cee346ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25eebbbbed104f48acb8291929ac2b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60fdffd40ca34431b90c1ad3e68084cd",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9eb00f3630534de9b4a0a5106f060013",
            "value": 4
          }
        },
        "2a32444f32c048dca764cca25b4eabd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aa8b2acec0942679b99134233f0e44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cc9c1e85ec54d2b81b4d4c1c93ab20e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dddc32d2978d4e22b411a3c09eae8ba4",
            "placeholder": "​",
            "style": "IPY_MODEL_2323f64bb2d448caa35cde8cee346ac4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3136f2e952f64b3b8805f216ce1f78d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d7ab3a8b31546f6aa0052856e6036fe",
              "IPY_MODEL_a558818de552481f86eb96c6b8904363",
              "IPY_MODEL_eede73f988864d8f845ca49b11f80351"
            ],
            "layout": "IPY_MODEL_5f90ef82fc694012942f569ae85584ad"
          }
        },
        "34996de2d4d04c60a9f29cfaf33f4b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_967d3ee39bc245b7a65c96594e5f38e5",
            "placeholder": "​",
            "style": "IPY_MODEL_8b8bb891412c420a980f9e41281e13c9",
            "value": " 4/4 [00:04&lt;00:00,  1.03s/it]"
          }
        },
        "3c4ea538ee3f499ba478253b0d1e667f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a504a18dbbb5445a8756dd3b67451f70",
              "IPY_MODEL_a28914d1b8a14b7ba8bc4e81eed6cae8",
              "IPY_MODEL_34996de2d4d04c60a9f29cfaf33f4b49"
            ],
            "layout": "IPY_MODEL_c6495124057b4d6ea522a10aac3a44d2"
          }
        },
        "3e435ec35b394d9a8ed3afe76cbe3b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf179b051a8a4fc380be321f4aaf7c6b",
              "IPY_MODEL_0eb1370d7c82457183d154800be61ea3",
              "IPY_MODEL_7c66af747fa540b082a5f99ac50c563f"
            ],
            "layout": "IPY_MODEL_d747c3cfbdb64c2d860c45bfc19af58f"
          }
        },
        "3ef938db60944f95ba0c248fe97fefad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a32444f32c048dca764cca25b4eabd4",
            "placeholder": "​",
            "style": "IPY_MODEL_4c924cc011cc418cb2bdb0f65b2009cc",
            "value": " 4/4 [00:04&lt;00:00,  1.02s/it]"
          }
        },
        "46041f75f7254fcda85cef2b0401d165": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492876adcfb5411386450b167c0cf98e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c924cc011cc418cb2bdb0f65b2009cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "507cb23020ac4a849d05c78953f83434": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51fbab3f5d2d4c71ab001afafb1c76fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52b12e8723704691850cd29080f406d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56b0d80c9692490b8ac5154c1790c204": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5707d2163bb848f3b59ad8e6e5321fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59eeaac680674844b574b03be58b3793": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a711718fffb4a39abc1b84fb655c792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_507cb23020ac4a849d05c78953f83434",
            "placeholder": "​",
            "style": "IPY_MODEL_52b12e8723704691850cd29080f406d1",
            "value": " 1/4 [00:01&lt;00:04,  1.51s/it]"
          }
        },
        "5f90ef82fc694012942f569ae85584ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "603090355d8746648a5e9952962f8712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71f8b13bc7234deb88d7d318452ae9e5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2501b2ceb0b47b9b2c45dd65c5b9f9b",
            "value": 4
          }
        },
        "60fdffd40ca34431b90c1ad3e68084cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6661bb0efd4c4f0f8be2a96aa61f8a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc96402e48c84ee1ae898aa0846aec40",
              "IPY_MODEL_603090355d8746648a5e9952962f8712",
              "IPY_MODEL_3ef938db60944f95ba0c248fe97fefad"
            ],
            "layout": "IPY_MODEL_ca284c8cb29344ea825dc8fbaac9cce8"
          }
        },
        "6c10be8fc8054c3bbf92c05f5dddc4ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7133b1db16c5459b8d097efc6757321c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71f8b13bc7234deb88d7d318452ae9e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73e3536f450c475da15c14a89fd3baa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c66af747fa540b082a5f99ac50c563f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afd50efe50b84c38bec39e4f95396940",
            "placeholder": "​",
            "style": "IPY_MODEL_806930ae0f0748808050c7f2aad68f93",
            "value": " 4/4 [00:04&lt;00:00,  1.04s/it]"
          }
        },
        "7d7ab3a8b31546f6aa0052856e6036fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59eeaac680674844b574b03be58b3793",
            "placeholder": "​",
            "style": "IPY_MODEL_73e3536f450c475da15c14a89fd3baa9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "806930ae0f0748808050c7f2aad68f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8305a78a8bf9432d8f7ac01795abea54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8566d51277414019a2907df83d17389c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86b7ef5d84f6413f8f3fe905304f2c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b8bb891412c420a980f9e41281e13c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9268369c051548d598f7f884c95fcb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92ffa3a0516a4542a73043a289374136": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95750c3367144c259dc4cbe545601bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "967d3ee39bc245b7a65c96594e5f38e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96829f52a8ae49798a1b6b9a239f8daa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97ef8e3cb10a42988472f302c386baf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc62bc8c3484e3caeb9fbd63badd5e4",
            "placeholder": "​",
            "style": "IPY_MODEL_492876adcfb5411386450b167c0cf98e",
            "value": " 4/4 [00:04&lt;00:00,  1.02s/it]"
          }
        },
        "9cc62bc8c3484e3caeb9fbd63badd5e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb00f3630534de9b4a0a5106f060013": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fcb8bb64c854024bcb2e41dfdcbbafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cc9c1e85ec54d2b81b4d4c1c93ab20e",
              "IPY_MODEL_25eebbbbed104f48acb8291929ac2b54",
              "IPY_MODEL_fff3f28324ae4322a3e705038bee305f"
            ],
            "layout": "IPY_MODEL_da1fb0b1072f4ad299bd522a4a0903b6"
          }
        },
        "a28914d1b8a14b7ba8bc4e81eed6cae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8305a78a8bf9432d8f7ac01795abea54",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00e9d9fd04da4c1f873816558bbe79ae",
            "value": 4
          }
        },
        "a453e3718a7542f481c2f0578d885101": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a47fe9856c66409b9f81eaa0f4df3b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c10be8fc8054c3bbf92c05f5dddc4ca",
            "placeholder": "​",
            "style": "IPY_MODEL_8566d51277414019a2907df83d17389c",
            "value": "Loading checkpoint shards:  25%"
          }
        },
        "a504a18dbbb5445a8756dd3b67451f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46041f75f7254fcda85cef2b0401d165",
            "placeholder": "​",
            "style": "IPY_MODEL_9268369c051548d598f7f884c95fcb9a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a558818de552481f86eb96c6b8904363": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92ffa3a0516a4542a73043a289374136",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7133b1db16c5459b8d097efc6757321c",
            "value": 4
          }
        },
        "afd50efe50b84c38bec39e4f95396940": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbf3b6e62a984e2485fa844fb4195dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2501b2ceb0b47b9b2c45dd65c5b9f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6495124057b4d6ea522a10aac3a44d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca055a839512487d88a541d1f9d5c154": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca284c8cb29344ea825dc8fbaac9cce8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb37b57587424f52b72d196708aa2be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a47fe9856c66409b9f81eaa0f4df3b97",
              "IPY_MODEL_0515815885a640fc88ed164265749d85",
              "IPY_MODEL_5a711718fffb4a39abc1b84fb655c792"
            ],
            "layout": "IPY_MODEL_ef92c244f3ca48ee900dee53a137862a"
          }
        },
        "cf179b051a8a4fc380be321f4aaf7c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca055a839512487d88a541d1f9d5c154",
            "placeholder": "​",
            "style": "IPY_MODEL_f335411ea2aa4d11bc2692c0779d8810",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d747c3cfbdb64c2d860c45bfc19af58f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da1fb0b1072f4ad299bd522a4a0903b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc96402e48c84ee1ae898aa0846aec40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96829f52a8ae49798a1b6b9a239f8daa",
            "placeholder": "​",
            "style": "IPY_MODEL_86b7ef5d84f6413f8f3fe905304f2c3b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dddc32d2978d4e22b411a3c09eae8ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de482c94baba4094934bb876f6d6590e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e52fe43fd31b4803babde96e0d0b7344": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eab3e0dd104d4f9396ffb684c7e8c811": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56b0d80c9692490b8ac5154c1790c204",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e52fe43fd31b4803babde96e0d0b7344",
            "value": 4
          }
        },
        "eede73f988864d8f845ca49b11f80351": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a453e3718a7542f481c2f0578d885101",
            "placeholder": "​",
            "style": "IPY_MODEL_51fbab3f5d2d4c71ab001afafb1c76fe",
            "value": " 4/4 [00:04&lt;00:00,  1.03s/it]"
          }
        },
        "ef92c244f3ca48ee900dee53a137862a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f335411ea2aa4d11bc2692c0779d8810": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbbb6ce1c12f4b739b87ebb4ed3a275a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fff3f28324ae4322a3e705038bee305f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95750c3367144c259dc4cbe545601bc5",
            "placeholder": "​",
            "style": "IPY_MODEL_fbbb6ce1c12f4b739b87ebb4ed3a275a",
            "value": " 4/4 [00:04&lt;00:00,  1.06s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
